{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOSlURBSJr+WVZ4Xhu4HUwD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"l8kWXpp5s6Gm","collapsed":true},"outputs":[],"source":["# importing oci library\n","import oci\n","\n","#setting up compartment id to compartment_id variable\n","compartment_id = \"use your own compartment id\" #if you don't find login to oracle cloud infrastructe->Profile->My Profile\n","# config_profile is the name of the profile in oci config file useful for authenticating and communicate with oci service\n","CONFIG_PROFILE = \"DEFAULT\"\n","# config variable is loading your oci cred and configuration settings from oci config file specified, that enable the script to authenticate your request\n","config = oci.config.from_file('/config', CONFIG_PROFILE)\n","\n","# service endpoint, this is the url where generative ai/ your request will reach. \"ap-hyderabad-1\" is the region\n","endpoint = \"https://inference.generativeai.ap-hyderabad-1.oci.oraclecloud.com\"\n","\n","# creating an instance of GenerativeAiInferenceClient, so that client lets you to interact with Generative AI Service\n","generative_ai_inference_client = oci.generative_ai_inference.GenerativeAiInferenceClient(config = config,service_endpoint=endpoint,retry_strategy = oci.retry.NoneRetryStrategy(), timeout = (10,240))\n","\n","# GenerateTextDetails() is a class of blueprint for generating sentences and paragrapth\n","generate_text_detail = oci.generative_ai_inference.models.GenerateTextDetails()\n","# creates new instance of CohereLlmInferenceRequest class. This class is used to specify the parameters for text generation request in Oracle Cloud Infrastructure\n","llm_inference_request = oci.generative_ai_inference.models.CohereLlmInferenceRequest()\n","# setting up the prompt\n","llm_inference_request.prompt = \"How does a microscope work?\"\n","# maximum number of tokens to be generated\n","llm_inference_request.max_tokens = 600\n","# setting tempearture = 1 to maintain the creativity and coherence(quakity)\n","llm_inference_request.temperature = 1\n","# setting it to 0 means giving your model a free pass to repeat itself\n","llm_inference_request.frequency_penalty = 0\n","# value of 0.75 means that the model will only consider the smallest set of tokens whose cumulative probability excceds 0.75\n","llm_inference_request.top_p = 0.75\n","# this creates an instance of OnDemandSerivingMode class with a specific model_id (is a unique identifier for a model in Oracle Cloud Infrastructure)\n","# this sets the serving_mode attribute of generate_text_detail object to OnDemandSeringMode instance.\n","generate_text_detail.serving_mode = oci.generative_ai_inference.models.OnDemandServingMode(model_id=\"ocidi.generativeaimodel.oci.ap-hyderabad-1.amaaaaaask7iceyahwal37hxwylnpbcncidim\")\n","\n","# inference_request attribute contains the parameters for the text generation request such as input text, max length and various control parameters\n","generate_text_detail.inference_request = llm_inference_request\n","\n","# setting compartment_id which is a unique identifier for compartment in OCI\n","generate_text_detail.compartment_id = compartment_id\n","\n","# calling generate_text method of generative_ai_inference_client object, passing generate_text_detail\n","generate_text_response = generative_ai_inference_client.generate_text(generate_text_detail)\n","#Print result\n","(\"********Generate Texts Result********\")\n","print(generate_text_response.data)"]},{"cell_type":"code","source":[],"metadata":{"id":"xcnqdzirGbD7"},"execution_count":null,"outputs":[]}]}